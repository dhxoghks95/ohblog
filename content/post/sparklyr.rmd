---
title: "Sparklyr로 R에서 Spark 분산처리 활용해 기계학습 진행하기 - 1. sparklyr 소개"
author : 오태환
date: 2020-08-30T17:15:21+09:00
tags : ["R", "Spark", "sparklyr", "dplyr", "tidyverse", "hadoop", "machine learning"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Sparklyr로 R에서 Spark 분산처리 활용해 기계학습 진행하기**
# **1. sparklyr 소개**

이 포스트는 Tidyverse Korea의

[sparklyr, dplyr, 그리고 기계학습](https://statkclee.github.io/bigdata/ml-sparklyr.html) 문서를 토대로 작성되었습니다.


# 0) 로컬 컴퓨터에 스파크 설치하기

[로컬 컴퓨터에 스파크 설치하기](https://statkclee.github.io/bigdata/ds-sparklyr.html)

를 보고 설치하면 된다. 윈도우의 경우 주의할 점은 환경변수 설정! 참고로 환경변수 경로를 변경할 때, 컴퓨터를 다시 시작해야 적용된다 ㅠㅠ 이것 때문에 하루종일 고생했다...

# 1) Rstudio에서 sparklyr 실행하기

sparklyr이라는 이름에서 눈치챘을 수도 있겠지만, 이 패키지는 Spark에서 dplyr을 사용할 수 있게 하는 패키지입니다. 

[제주 빅데이터 경진대회](https://github.com/dhxoghks95/2020_jeju_creditcard)를 준비하면서 가장 많이 궁금했던 점이 "R로 어떻게 큰 데이터를 다룰 수 있을까" 라는 질문이었습니다. 분명 R은 Tidyverse를 필두로 한 강력한 데이터 정제 패키지를 가지고 있습니다. 하지만 데이터가 조금만 커져도 처리 속도가 굉장히 느려진다는 단점이 있죠. 그 단점을 해결하기 위해 만들어진 것이 바로 Spark의 빠른 분산 처리 시스템과 R의 데이터 정제 능력을 결합한 Sparkly 패키지입니다. 이를 활용함으로서 빅데이터를 R에서도 다룰 수 있게 됩니다.

```{r}
# install.packages("sparklyr")
# install.packages("tidyverse")

library(sparklyr)
library(tidyverse)
```

우선 패키지를 설치 후 Import합니다

그리고 스파크 클러스터에 연결합시다.

```{r}

sc <-spark_connect(master="local")

# 자 이제 이 sc(spark cluster)를 통해 스파크에 접속할 것입니다.


spark_version(sc=sc)

# 버전이 잘 나오면 잘 연결된것입니다.

# 스파크 연결해제
# spark_disconnect(sc=sc)

```

# 2) Spark에 csv파일 불러오기

sparklyr에도 tidyverse의 readr::read_csv()와 같이 데이터를 불러오는 함수가 있습니다. spark_read_csv()함수죠. 참고로 iris와 같이 R에 내장되어 있는 데이터프레임의 경우에는 copy_to()함수를 쓰면 됩니다. 우리는 [kaggle 2015 Flight Delays and Cancellations](https://www.kaggle.com/usdot/flight-delays)에서 다운받을 수 있는 filight.csv파일을 통해 실습을 진행하도록 하겠습니다.

```{r}
flight_tbls = spark_read_csv(sc, name = "flights", path = "C:/Users/dhxog/Downloads/810_1496_bundle_archive/flights.csv")
# sc로 연결할 spark cluster를 넣고, table의 이름을 지어서 name으로 넣습니다. 그리고 path에 경로를 넣으면 됩니다.

# 내장 데이터프레임의 경우

# iris_tbls = copy_to(sc, iris)

src_tbls(sc)
```

sc에 flights라는 이름의 데이터 프레임 테이블이 들어온 것을 볼 수 있습니다!

![spark_cluster](https://user-images.githubusercontent.com/57588650/91659225-aa24e880-eb09-11ea-9f2c-648375a83a64.png)

출처 : https://statkclee.github.io/bigdata/ml-sparklyr.html

이와 같이 로컬 컴퓨터에 있는 flights.csv를 Spark Cluster에 업로드 시킨것으로 이해하시면 됩니다.

# 3) Spark Cluster에 있는 파일을 Tibble로 불러오기

자 이전 과정에서 우리는 Spark Cluster에 csv 파일을 올렸습니다. 이제 그 데이터를 어떻게 활용해야 할까요? 바로 tibble로 가져오는 것입니다. tibble은 매우 빠르고 작은 데이터를 반환하기 때문에 데이터를 분석하기에 굉장히 용이합니다. 그러면서 실제 데이터는 Spark Cluster에 존재하게 됩니다.

```{r}
flights_tbl = tbl(sc, "flights")
# Spark Cluster에서 "flights"라는 이름의 dataframe을 tibble로 가져옵니다

dim(flights_tbl)
```

불러온 데이터의 크기를 확인해봅시다
```{r}
#install.packages("pryr")
pryr::object_size(flights_tbl)
```

pryr 패키지의 object_size함수를 사용하면 데이터의 크기를 알 수 있습니다

자 이제 불러온 데이터를 조금 봐볼까요?

```{r}
print(flights_tbl, n = 10)
```

자료구조도 확인해봅시다

```{r}
glimpse(flights_tbl)
```

# 4) 불러온 데이터를 dplyr로 만져보기

자 이제 Spark DataFrame을 tbl 명령어를 사용해 tibble로 불러왔으니 dplyr패키지의 멋진 함수들을 모두 쓸 수 있습니다.
하나씩 써보죠!

## 4-1) select
```{r}
flights_tbl %>% dplyr::select(YEAR, MONTH, DAY, AIRLINE, FLIGHT_NUMBER, SCHEDULED_ARRIVAL)

```

## 4-2) filter
```{r}
flights_tbl %>% dplyr::select(YEAR, MONTH, DAY, AIRLINE, FLIGHT_NUMBER, SCHEDULED_ARRIVAL) %>% dplyr::filter(AIRLINE == 'AA')
```

## 4-3) arrange
```{r}
flights_tbl %>% 
  dplyr::select(YEAR, MONTH, DAY, AIRLINE, FLIGHT_NUMBER, SCHEDULED_ARRIVAL) %>% 
  dplyr::filter(AIRLINE == 'AA') %>%
  dplyr::arrange(desc(FLIGHT_NUMBER), SCHEDULED_ARRIVAL)
```

## 4-4) mutate
```{r}
flights_tbl %>% 
  dplyr::select(YEAR, MONTH, DAY, AIRLINE, FLIGHT_NUMBER, SCHEDULED_ARRIVAL) %>% 
  dplyr::filter(AIRLINE == 'AA') %>%
  dplyr::arrange(desc(FLIGHT_NUMBER), SCHEDULED_ARRIVAL) %>%
  dplyr::mutate(AIR_NUMBER = paste0(AIRLINE, '_', FLIGHT_NUMBER))
```

## 4-5) summerize
```{r}
flights_tbl %>% 
  dplyr::select(YEAR, MONTH, DAY, AIRLINE, FLIGHT_NUMBER, SCHEDULED_ARRIVAL) %>% 
  dplyr::filter(AIRLINE == 'AA') %>%
  dplyr::arrange(desc(FLIGHT_NUMBER), SCHEDULED_ARRIVAL) %>%
  dplyr::mutate(AIR_NUMBER = paste0(AIRLINE, '_', FLIGHT_NUMBER)) %>%
  dplyr::summarise(MEAN_ARRIVAL = mean(SCHEDULED_ARRIVAL))
```

# 5) dplyr 고급 기능 사용하기

앞에서 사용한 함수들 안에 parameter를 더 넣고, count, group_by등의 함수도 쓸 수 있습니다.

## 5-1) select - starts_with

```{r}
flights_tbl %>% dplyr::select(YEAR, MONTH, DAY, FLIGHT_NUMBER, starts_with("AIR"))
```
select 함수 안에 start_with("AIR")를 쓰면 AIR로 시작하는 모든 column들을 선택할 수 있습니다

## 5-2) distinct

```{r}
flights_tbl %>% distinct(AIRLINE)
```
distinct로 unique값을 출력할 수 있습니다

## 5-3) count

```{r}
flights_tbl %>% dplyr::count(AIRLINE, sort = TRUE)
```
count함수를 사용하면 factor들의 갯수를 구할 수 있습니다.

## 5-4) group_by

```{r}
flights_tbl %>% 
  dplyr::select(YEAR, MONTH, DAY, AIRLINE, FLIGHT_NUMBER, SCHEDULED_ARRIVAL) %>% 
  dplyr::arrange(desc(FLIGHT_NUMBER), SCHEDULED_ARRIVAL) %>%
  dplyr::mutate(AIR_NUMBER = paste0(AIRLINE, '_', FLIGHT_NUMBER)) %>%
  dplyr::group_by(AIRLINE) %>%
  dplyr::summarise(MEAN_ARRIVAL = mean(SCHEDULED_ARRIVAL))
```
## 5-5) explain

dplyr은 SQL쿼리를 참고해서 만들어진 함수입니다. explain() 함수를 사용하면 사용한 dplyr함수와 동일한 값을 반환하는 SQL 쿼리를 출력할 수 있습니다.

```{r}
flights_tbl %>% 
  dplyr::select(YEAR, MONTH, DAY, AIRLINE, FLIGHT_NUMBER, SCHEDULED_ARRIVAL) %>% 
  dplyr::arrange(desc(FLIGHT_NUMBER), SCHEDULED_ARRIVAL) %>%
  dplyr::group_by(AIRLINE) %>%
  dplyr::summarise(MEAN_ARRIVAL = mean(SCHEDULED_ARRIVAL)) %>%
  explain()
```

```{r}
#install.packages("DBI")
DBI::dbGetQuery(sc, 
"SELECT `AIRLINE`, AVG(`SCHEDULED_ARRIVAL`) AS `MEAN_ARRIVAL`
FROM (SELECT *
FROM (SELECT `YEAR`, `MONTH`, `DAY`, `AIRLINE`, `FLIGHT_NUMBER`, `SCHEDULED_ARRIVAL`
FROM `flights`) `dbplyr_053`
ORDER BY `FLIGHT_NUMBER` DESC, `SCHEDULED_ARRIVAL`) `dbplyr_054`
GROUP BY `AIRLINE`")
```
위와 같이 DBI 패키지의 dbGetQuery함수로 위에서 출력한 SQL 쿼리를 복사 붙여넣기함으로써 Spark Cluster에서 SQL 쿼리로 직접 조작된 데이터를 가져올 수 있습니다.

# 6) 데이터프레임 변환과 중간결과 저장

지금까지 spark_read_csv() 또는 copy_to()를 통해 Spark Cluster에 R 데이터프레임을 보내고 Spark Cluster 데이터프레임을 dplyr을 통해 출력해보았습니다. 
반대로 Spark Cluster 데이터프레임을 R 데이터프레임으로 변환하려면 collect() 함수를 사용해야합니다.

dplyr 파이프 연산자를 통해 데이터를 조작할 때, 코드가 길어질 경우에는 중간에 세션이 다운되는 경우가 흔합니다. 그렇기 때문에 compute 함수를 사용해 Spark Cluster에 중간결과를 저장해 안정적으로 후속작업을 진행할 수 있습니다.

## 6-1) R 데이터프레임으로 변환

```{r}
flights_df = flights_tbl %>% 
  dplyr::select(YEAR, MONTH, DAY, starts_with("AIR")) %>%
  collect()

class(flights_df)
```
## 6-2) 중간결과 저장

```{r}
flights_air = flights_tbl %>% 
  dplyr::select(YEAR, MONTH, DAY, starts_with("AIR")) %>%
  compute("flight_air")

src_tbls(sc)
```
Spark Cluster에 flight_air라고 이름붙인 중간결과가 업로드 된 것을 볼 수 있습니다.


