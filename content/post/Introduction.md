---
title: Bayesian Method with TensorFlow - 1. Introduction
author: 오태환
date: 2020-08-26T23:15:00+09:00
categories: ["Bayesian Method with TensorFlow"]
tags: ["Bayesian", "TensorFlow", "Python"]
---

# **Bayesian Method with TensorFlow**

# 1. Introduction

이 포스트는 Cam Davison Pilon의 

Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
([깃헙링크](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers))

를 혼자 공부하기 위한 것입니다.

Tensorflow 를 통해 bayesian method를 python으로 구현하는 공부가 될 것입니다. 

모든 게시물은 Google Colab으로 작성되었으며

코드는 위의 깃헙 링크에서 원본을 보실 수 있습니다.

천천히 시작해보도록 합시다!

### **베이지안 추론의 철학**

> 당신은 유능한 프로그래머지만, 여전히 당신의 코드에 버그가 발생한다. 어렵게 알고리즘을 고치고 일단 쉬운 예제로 테스트 해본다. 그 테스트가 통과한다면 점점 더 어려운 예제로 테스트한다. 그 테스트가 성공할 수록 당신은 그 코드에 더이상 버그가 없다고 믿기 시작한다.

만약 당신이 이러한 방식으로 생각한다면, 이미 당신은 베이지안의 방식대로 생각하는 것입니다! 베이지안 추론은 간단하게 말하지면 새로운 증거들을 고려해 당신의 믿음을 업데이트 해나가는 것입니다. 베이지안은 결과에 대해 확신하는 일은 드물지만, 자신감있게 말할 수는 있습니다. 위의 예시처럼 우리는 모든 상황에서 테스트 해보지 않는 한 코드가 100% 버그가 없다고 말할 수는 없습니다. 하지만 실제론 모든 상황에 대해 테스트 할 수 없기 때문에 최대한 많은 상황에서 테스트 해보고 이것이 성공한다면 더 큰 자신감을 얻을 수 있습니다(확신할 순 없지만...).  베이지안 추론은 이와 비슷합니다. 비록 모든 대안들을 모두 실행해보고 가설을 확신할 순 없지만, 계속해서 우리의 믿음을 업데이트해가는 것입니다.

### **베이지안의 마음가짐**

베이지안 추론은 불확실성을 유지한다는 점에서 전통적인 통계적인 추론 방식과는 다릅니다. 불확실성을 유지한다? 우리는 통계학을 배울 때 무작위하게 보이는 것에서 무언가 확실한 것을 뽑아내는 것이라고 배웠기 때문에(예를 들면 p-value가 0.05보다 낮으면 귀무가설을 기각한다던지) 처음 이것을 들을 땐 이상한 통계 방법론 아닌가? 라고 생각할 수 있습니다. 하지만 우리는 이미 베이지안의 방식대로 생각하고 있습니다!

베이지안들은 확률을 어떤 사람이 사건에 대해 가진 믿음이라고 해석합니다. 즉 그 확률이 몇 퍼센트의 확률로 일어날지에 대해 얼마나 자신 있는지를 말하는거죠. 사실 이게 자연스러운 확률에 대한 생각입니다 여러분도 다 이렇게 생각할걸요?

더 설명을 해보자면, 확률이 무엇인지에 대한 생각은 두 가지가 있습니다. 첫 번째가 바로 위에서 설명한 베이지안이고 두 번째는 빈도주의(Frequentist)입니다. 빈도주의자들은 확률을 long-run frequency, 즉 긴 시간동안 발생하는 사건 중에 특정한 사건이 몇 번 일어났는지가 확률이란거죠. 예를 들면 50년 동안 1만건의 비행 중에 10건의 사고가 일어났으니 비행기 사고가 발생할 확률은 0.1%다! 라고 생각하는거죠. 논리적인 사고 방식이라고 생각하실 수 있습니다. 하지만 세상은 항상 저렇게 수많은 데이터를 가지고 있지 않습니다. 대통령 선거를 생각해봅시다. 19대 대통령 선거는 딱 한 번이었죠? 이런 경우 빈도주의론자들은 특정 대통령 후보가 당선될 확률을 말할 수 있을까요?

베이지안은 좀 더 직관적입니다. 베이지안들은 확률을 믿음의 정도라고 생각합니다. 간단하게 확률은 한 사람의 견해라는거죠. 한 사람이 어느 사건이 발생할거란 자신이 없으면 그 사건이 발생할 확률은 낮다고 생각할거고 자신감 만땅이면 높은 확률로 사건이 일어날 것이라고 믿을것입니다. 그런데 이 믿음이 빈도주의와 완전 다른건 아닙니다. 당연히 다른 정보들은 차치하고 1만번의 비행 중 10건의 사고가 났다는 정보만 가지고 있다면 0.1%의 확률로 사고가 일어날 것이라고 '믿음'을 가질 수 있습니다. 하지만 누가 대통령에 당선될 것인가 같은 빈도주의자들은 대답 못할 것을 베이지안은 특정 후보가 몇 퍼센트의 확률로 대통령에 당선될 것 같다고 말할 수 있죠.

흥미로운건 지금까지 제가 '한 사람'의 믿음이라고 표현한겁니다. 일반적인 생각이 아니라요. 즉 의견이 갈릴 여지가 있다는겁니다. 사람간의 생각이 다른건 자연스러운 일이니까요. 사람들의 의견이 갈린다고 해서 누군가가 틀렸다는건 아닙니다. 밑의 예로 개인의 믿음과 확률간의 상관관계에 대해 설명해보겠습니다.

* 제가 동전을 튕기고 같이 결과를 예측해봅시다. 우린 그 동전이 정상적인 동전이라면 당연히 앞면이 나올 확률은 50%라고 생각할것입니다. 자 이제 저는 동전을 튕기고 저만 그 결과를 안다고 가정합시다. 당연히 저는 결과를 알기에 앞면 혹은 뒷면이라고 100% 확신할 수 있습니다. 하지만 당신은 결과를 모르고 또 믿음을 바꾼다고 해서 결과 도 바꿀 수 있는건 아니기 때문에 여전히 50%의 확률로 앞면 혹은 뒷면이 나올것이란 믿음을 바꾸지 않을 것입니다. 자 이렇게 되면 저의 확률과 당신의 확률은 달라지게 되겠죠? 

* 당신의 코드는 버그가 있을 수도 없을 수도 있습니다. 즉 버그의 존재에 대한 가능성만 있을 뿐 확신할 순 없단거죠.

* 한 환자가 세 가지의 증상을 가지고 있습니다. 그리고 그 증상들을 가질 수 있는 질병은 여러가지가 있는데, 그 환자는 한 가지의 질병만을 가지고 있다고 합시다. 한 의사는 그게 어떤 병일거라고 믿음을 가질 수 있지만 다른 의사는 다른 질병이라고 생각할 수도 있죠.

이렇게 믿음을 확률로 생각하는건 사람들에게 자연스러운 일입니다. 우리는 세상과 지속적으로 상호작용하면서 부분적인 진실만을 마주하지만, 증거들을 모아가면서 믿음을 형성합니다. 이와는 반대로, 당신은 빈도주의자와 같이 생각하도록 훈련받아야 합니다.

우리는 보통 어떤 사건 $A$에 대한 믿음을 $P(A)$라고 표기합니다. 이제 이것을 'Prior Probability(사전 확률)'이라고 부릅시다.


위대한 경제학자이자 사상가인 케인즈는 이렇게 말했습니다. '팩트가 바뀐다면, 저는 제 생각을 바꿉니다. 당신은 뭘 하나요?' 이 말은 베이지안들이 증거를 본 뒤 그들의 생각을 업데이트하는 방식을 보여줍니다. 만약 그 증거가 원래 생각에 반한다고 해도, 그 증거는 무시될 수 없습니다. 우리는 이러한 업데이트된 믿음을 $P(A|X)$ 라고 쓰겠습니다. 이는 $X$라는 증거가 주어졌을 때, $A$가 일어날 확률로 설명할 수 있습니다. 우리는 이러한 업데이트된 믿음을 앞에 사전 확률과 대비되게 'Posterior Probability(사후 확률)'이라고 부르도록 합시다. 예를 들면 위의 예시에서 증거가 주어진 후 사후 확률(사후 믿음)이 어떨지 생각해봅시다.

1.  $P(A)$ : 동전의 앞면이 나올 확률은 50%이다

    $P(A|X)$ : 동전을 던지고 앞면이 나온 것을 확인한다. 이 정보를 얻었으면 당연히 앞면 나올 확률은 100%고 뒷면 나올 확률은 0%로 할당하게 된다.

2.  $P(A)$ : 이러한 복잡한 코드는 버그가 있을 확률이 크다

    $P(A|X)$ : 이 코드는 모든 X번의 테스트들을 통과했다. 물론 버그가 있을 순 있겠지만, 이제는 그 가능성이 매우 낮아졌다.

3.  $P(A)$ : 그 환자는 어떤 질병도 있을 가능성이 있다.

    $P(A|X)$ : 혈액 검사 결과 X라는 증거를 가지고 특정한 질병을 고려대상에서 제외했다.

보면 알 수 있지만, 위의 예시들은 사전 믿음을 완전히 버리진 않았지만 증거 $X$를 보고 가중치를 재조정했습니다. (예를 들면 특정한 믿음에 더 큰 가중치를 주는거죠)

우리가 가지고 있는 사전 믿음이 아주 틀릴 수도 있습니다. 그런데 데이터, 증거 또는 다른 정보를 탐색하고 우리의 믿음을 업데이트 한다면, 우리의 추측은 덜 틀리게 되겠죠?

### **현실 세계에서의 베이지안 추론**

만약 베이지안과 빈도주의가 통계적인 문제를 받아 결과를 출력하는 프로그래밍 함수라면, 둘은 다른 결과를 사용자에게 출력할 것입니다. 빈도주의 추론 함수는 추정치를 하나의 숫자로 출력할 것입니다(예를 들면 평균 같은). 그런데 베이지안 함수는 확률을 출력할 것입니다.

에를 들면 위에 있는 코드 속의 버그 문제에서 빈도주의 추론 함수는 "내 코드가 모든 X번의 테스트를 통과했어! 그럼 내 코드는 버그가 없는걸까?" 라는 질문에 "응!" 이라는 하나의 값을 출력할 것입니다. 그러나 베이지안 함수는 "보통 내 코드엔 버그가 있는데, 이번엔 모든 X번의 테스트를 통과했어! 그럼 내 코드는 버그가 없는걸까?" 라는 질문에 이와는 다른 답을 출력할 것입니다.

> 80%의 확률로 버그가 없고 20%의 확률로 버그가 있어!

이것은 빈도주의 함수와는 전혀 다른 답입니다. 우선 질문데 다른 점이 있다는걸 확인해봅시다. "보통 내 코드엔 버그가 있는데"라는건 바로 사전 믿음을 의미합니다. 사전 믿음이라는 모수를 추가함으로서 우리는 베이자안 함수가 우리의 믿음을 반영한다고 말할 수 있게 됩니다. 기술적으로 이러한 모수는 넣어도 되고 안넣어도 됩니다. 그런데 이걸 뺀다면 함수의 출력물은 다를 것입니다.

### 증거들을 통합하기

우리가 점점 증거들을 수집할 수록, 우리의 사전 믿음은 새로운 증거들에 의해 점점 희미해져갑니다. 예를 들면 당신이 "내일 태양은 터질거야!"라고 바보같은 사전 믿음을 가지고 있다면, 매일 당신은 틀렸다고 판명날것이고 어떤 추론이 당신의 믿음을 정정해주거나 최소한 당신의 믿음을 더 나아지기를 바랄 것입니다. 베이지안 추론은 이러한 믿음을 수정해줍니다

$N$이 우리가 가지고 있는 증거들의 갯수라고 해봅시다. 우리가 무한대의 증거들을 수집할수록, 우리의 베이지안 추론의 결과는 높은 확률로 빈도주의 추론의 결과와 일치합니다. 따라서 큰 $N$에 대해서 통계적인 추론(빈도주의자들의)은 나름 객관적입니다. 하지만 작은 $N$에 대해서는 훨씬 불안정합니다. 빈도주의자들의 추정은 더 큰 분산과 더 넓은 신뢰구간을 가지게 되기 때문이죠. 이것이 베이지안 분석이 뛰어난 부분입니다. 사전 믿음과 사후 확률을 도입함으로서(숫자로 딱 떨어지는 것 대신), 우리는 작은 $N$개의 데이터들이 가지고 있는 불안전성을 반영하게 됩니다. 

어떤 사람은 이렇게 생각할 수 있습니다. "$N$이 충분히 크다면 어짜피 빈도주의나 베이지안이나 별 다를거 없다는데 그럼 컴퓨터로 값을 구하기 더 간단한 빈도주의적인 방법 쓰면 되는거 아냐?" 

이런 생각을 가지고 있는 사람들은 그러한 결정을 내리기 전에 Andrew Gelman이 2005년에 한 말[1]을 보고 다시 고려해봅시다.

> 표본의 크기는 절대 크지 않습니다. 만일 $N$이 충분히 정확한 추정을 만들기에 너무 작다면, 당신은 더 많은 데이터를 필요로 할 것입니다(또는 더 많은 가정을 해야할 것입니다). 그러나 $N$이 "적절히 크다면", 당신은 더 많은 것을 알기 위해 데이터를 나누기 시작할 것입니다(예를 들면 여론조사에서 전국에 대해 좋은 추정치를 내놨다면 당신은 남녀, 지역, 나이 별 등의 나눠진 그룹에 대한 결과에 대해도 추정할 수 있을 것입니다). 당신이 충분한 데이터를 얻었다고 생각한다고 해도 결국에는 더 많은 데이터를 필요로 하는 문제를 마주할 것이기 때문에 $N$은 절대 충분하지 않습니다.

### **그럼 빈도주의자들의 방법이 틀렸단건가요?**

그건 아닙니다.

빈도주의자들의 방법은 여전히 많은 분야에서 유용합니다. Least Squares나 선형 회귀, LASSO regression, 그리고 EM 알고리즘 등은 모두 강력하고 빠릅니다. 베이지안 방법론들은 이러한 테크닉들로는 접근하지 못하는 문제들을 풀거나, 더 유연한 모델을 만듦으로서 이들을 보완합니다.

### 빅데이터에서의 활용

모순적으로, 빅데이터의 예측을 위한 분석들은 상대적으로 쉬운 알고리즘들로 해결 가능합니다[2][3]. 따러서 우리는 빅데이터 예측의 어려운 점은 알고리즘이 아니라 컴퓨터로 빅데이터를 저장하고 활용하는 것이라고 말합니다.(또한 위에서 소개한 Gelman의 말 처럼 "내가 진짜 빅 데이터를 가지고 있는게 맞나?"라는 질문을 할 수도 있겠죠)

더 어려운 분석 문제는 중간 사이즈의 데이터나 특히 어려운 정말 작은 데이터를 분석하는 것입니다. 위의 Gelman이 말한 것과 비슷하게 말해보자면, 만일 우리가 빅데이터 문제들은 쉽게 해결 가능할 만큼 큰 데이터를 가지고 있다면 우리는 이제 그렇게 크지 않은 데이터에 더 큰 관심을 가져야 하지 않을까요?

## References


[1] Gelman, Andrew. N.p.. Web. 22 Jan 2013. N is never large enough

[2] Norvig, Peter. 2009. The Unreasonable Effectiveness of Data.

[3] Jimmy Lin and Alek Kolcz. Large-Scale Machine Learning at Twitter. Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data (SIGMOD 2012), pages 793-804, May 2012, Scottsdale, Arizona.

